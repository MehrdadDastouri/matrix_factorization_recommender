{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNawgfn5f1W5dGCVIaMMcnT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MehrdadDastouri/matrix_factorization_recommender/blob/main/matrix_factorization_recommender.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate synthetic user-item rating data\n",
        "def generate_data(num_users=10, num_items=15, sparsity=0.7):\n",
        "    \"\"\"\n",
        "    Generate a synthetic user-item rating matrix with sparsity.\n",
        "\n",
        "    Args:\n",
        "    - num_users: Number of users.\n",
        "    - num_items: Number of items.\n",
        "    - sparsity: Proportion of missing ratings.\n",
        "\n",
        "    Returns:\n",
        "    - ratings: User-item rating matrix (numpy array).\n",
        "    \"\"\"\n",
        "    np.random.seed(42)\n",
        "    ratings = np.random.randint(1, 6, size=(num_users, num_items)).astype(float)  # Ratings between 1 and 5\n",
        "    mask = np.random.rand(*ratings.shape) < sparsity  # Create missing values\n",
        "    ratings[mask] = 0  # Set missing ratings to 0\n",
        "    return ratings\n",
        "\n",
        "# Generate synthetic data\n",
        "num_users = 10\n",
        "num_items = 15\n",
        "sparsity = 0.7\n",
        "ratings = generate_data(num_users, num_items, sparsity)\n",
        "print(\"User-Item Rating Matrix:\")\n",
        "print(ratings)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "ratings_tensor = torch.tensor(ratings, dtype=torch.float32)\n",
        "\n",
        "# Split data into train and test sets\n",
        "def train_test_split_matrix(ratings, test_ratio=0.2):\n",
        "    \"\"\"\n",
        "    Split the rating matrix into training and testing sets.\n",
        "\n",
        "    Args:\n",
        "    - ratings: User-item rating matrix.\n",
        "    - test_ratio: Proportion of ratings to include in the test set.\n",
        "\n",
        "    Returns:\n",
        "    - train: Training rating matrix.\n",
        "    - test: Testing rating matrix.\n",
        "    \"\"\"\n",
        "    train = ratings.copy()\n",
        "    test = np.zeros_like(ratings)\n",
        "    for user in range(ratings.shape[0]):\n",
        "        non_zero_indices = ratings[user, :].nonzero()[0]\n",
        "        test_indices = np.random.choice(non_zero_indices, size=int(len(non_zero_indices) * test_ratio), replace=False)\n",
        "        train[user, test_indices] = 0\n",
        "        test[user, test_indices] = ratings[user, test_indices]\n",
        "    return train, test\n",
        "\n",
        "train_ratings, test_ratings = train_test_split_matrix(ratings, test_ratio=0.2)\n",
        "train_ratings_tensor = torch.tensor(train_ratings, dtype=torch.float32)\n",
        "test_ratings_tensor = torch.tensor(test_ratings, dtype=torch.float32)\n",
        "\n",
        "# Define the Matrix Factorization model\n",
        "class MatrixFactorization(nn.Module):\n",
        "    def __init__(self, num_users, num_items, latent_dim):\n",
        "        super(MatrixFactorization, self).__init__()\n",
        "        self.user_factors = nn.Embedding(num_users, latent_dim)  # User latent factors\n",
        "        self.item_factors = nn.Embedding(num_items, latent_dim)  # Item latent factors\n",
        "\n",
        "    def forward(self, user, item):\n",
        "        user_embedding = self.user_factors(user)\n",
        "        item_embedding = self.item_factors(item)\n",
        "        return (user_embedding * item_embedding).sum(1)  # Dot product of user and item factors\n",
        "\n",
        "# Initialize the model\n",
        "latent_dim = 5\n",
        "model = MatrixFactorization(num_users, num_items, latent_dim)\n",
        "criterion = nn.MSELoss()  # Loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)  # Optimizer\n",
        "\n",
        "# Prepare training data\n",
        "train_indices = torch.nonzero(train_ratings_tensor > 0)  # Indices of non-zero ratings\n",
        "train_users = train_indices[:, 0]\n",
        "train_items = train_indices[:, 1]\n",
        "train_ratings = train_ratings_tensor[train_users, train_items]\n",
        "\n",
        "# Training loop\n",
        "epochs = 100\n",
        "train_losses = []\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    predictions = model(train_users, train_items)\n",
        "    loss = criterion(predictions, train_ratings)\n",
        "    train_losses.append(loss.item())\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Evaluate the model\n",
        "model.eval()\n",
        "test_indices = torch.nonzero(test_ratings_tensor > 0)\n",
        "test_users = test_indices[:, 0]\n",
        "test_items = test_indices[:, 1]\n",
        "test_ratings = test_ratings_tensor[test_users, test_items]\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_predictions = model(test_users, test_items)\n",
        "test_loss = criterion(test_predictions, test_ratings)\n",
        "print(f\"Test Loss (MSE): {test_loss.item():.4f}\")\n",
        "\n",
        "# Visualize the original, train, and predicted matrices\n",
        "predicted_ratings_tensor = torch.zeros_like(ratings_tensor)\n",
        "with torch.no_grad():  # Disable gradient calculation\n",
        "    for user in range(num_users):\n",
        "        for item in range(num_items):\n",
        "            predicted_ratings_tensor[user, item] = model(\n",
        "                torch.tensor([user]), torch.tensor([item])\n",
        "            )\n",
        "\n",
        "print(\"\\nOriginal Ratings:\")\n",
        "print(ratings)\n",
        "print(\"\\nTrain Ratings:\")\n",
        "print(train_ratings)\n",
        "\n",
        "# Fix: Use .detach() before converting to NumPy\n",
        "print(\"\\nPredicted Ratings:\")\n",
        "print(predicted_ratings_tensor.detach().numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrQ1SxOqGj-X",
        "outputId": "6dbc7c65-4c11-4dcf-f9d7-1213dc336bdd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User-Item Rating Matrix:\n",
            "[[0. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 5. 0. 1. 0. 3. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 4. 0. 4. 2. 0.]\n",
            " [0. 0. 5. 0. 0. 4. 4. 0. 0. 3. 0. 0. 2. 0. 2.]\n",
            " [0. 0. 0. 0. 0. 4. 0. 0. 4. 4. 0. 5. 0. 0. 5.]\n",
            " [2. 1. 0. 4. 4. 5. 1. 0. 0. 0. 1. 1. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 2. 2. 0. 0. 0. 0. 0. 0.]\n",
            " [5. 0. 4. 0. 0. 0. 0. 5. 0. 0. 0. 2. 0. 1. 0.]\n",
            " [0. 4. 5. 3. 0. 0. 0. 4. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [3. 4. 0. 0. 0. 0. 0. 0. 0. 4. 0. 4. 0. 0. 0.]]\n",
            "Epoch [10/100], Loss: 13.3372\n",
            "Epoch [20/100], Loss: 10.9463\n",
            "Epoch [30/100], Loss: 9.0327\n",
            "Epoch [40/100], Loss: 7.4371\n",
            "Epoch [50/100], Loss: 6.0584\n",
            "Epoch [60/100], Loss: 4.8510\n",
            "Epoch [70/100], Loss: 3.8064\n",
            "Epoch [80/100], Loss: 2.9344\n",
            "Epoch [90/100], Loss: 2.2377\n",
            "Epoch [100/100], Loss: 1.6939\n",
            "Test Loss (MSE): 14.4191\n",
            "\n",
            "Original Ratings:\n",
            "[[0. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 5. 0. 1. 0. 3. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 4. 0. 4. 2. 0.]\n",
            " [0. 0. 5. 0. 0. 4. 4. 0. 0. 3. 0. 0. 2. 0. 2.]\n",
            " [0. 0. 0. 0. 0. 4. 0. 0. 4. 4. 0. 5. 0. 0. 5.]\n",
            " [2. 1. 0. 4. 4. 5. 1. 0. 0. 0. 1. 1. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 2. 2. 0. 0. 0. 0. 0. 0.]\n",
            " [5. 0. 4. 0. 0. 0. 0. 5. 0. 0. 0. 2. 0. 1. 0.]\n",
            " [0. 4. 5. 3. 0. 0. 0. 4. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [3. 4. 0. 0. 0. 0. 0. 0. 0. 4. 0. 4. 0. 0. 0.]]\n",
            "\n",
            "Train Ratings:\n",
            "tensor([5., 5., 1., 3., 1., 4., 4., 2., 5., 4., 3., 2., 2., 4., 4., 4., 5., 1.,\n",
            "        4., 4., 5., 1., 1., 1., 1., 2., 2., 5., 4., 5., 2., 4., 5., 3., 4., 3.,\n",
            "        4., 4., 4.])\n",
            "\n",
            "Predicted Ratings:\n",
            "[[ 2.0741947e+00  2.3875103e+00  4.7432270e+00 -5.5390167e-01\n",
            "  -3.0752912e+00 -3.2003307e-01 -3.0653603e+00  1.1622658e+00\n",
            "   7.4872977e-01 -1.2277201e+00  1.4261467e+00 -5.4546559e-01\n",
            "   3.7109494e+00 -1.1068639e+00 -2.8557730e-01]\n",
            " [-1.1897664e+00 -1.7303077e+00  7.2190875e-01  1.7869452e+00\n",
            "  -7.8554624e-01  1.3453758e+00  4.3542118e+00 -1.4363203e+00\n",
            "   9.5681298e-01  1.5607040e+00  3.1833868e+00  9.2492795e-01\n",
            "  -2.0862818e+00 -9.8834777e-01  1.7789949e+00]\n",
            " [ 1.7854085e+00  1.5248549e+00  1.5891950e+00 -1.7836615e+00\n",
            "  -2.2307830e-01  1.0469410e+00 -1.3942003e+00  1.6014743e+00\n",
            "   9.6195674e-01 -8.4776431e-04  4.1086345e+00  5.9217823e-01\n",
            "   3.9106188e+00  1.8522050e+00 -4.2197871e+00]\n",
            " [-7.1497679e-01 -8.9412242e-01  2.7957997e+00  6.8473154e-01\n",
            "  -2.6324065e+00  2.1542361e+00  7.9123968e-01 -1.8021489e+00\n",
            "   8.9414251e-01  1.1539234e-02  4.7670050e+00 -1.6804173e+00\n",
            "   1.4132023e+00 -3.1883211e+00  2.0064833e+00]\n",
            " [-1.3006926e-02 -3.0338724e+00 -2.3112178e+00  5.6047001e+00\n",
            "   3.3260427e+00  3.7287366e+00  6.7837162e+00 -3.1402862e-01\n",
            "   4.0992575e+00  4.8273420e+00  7.4457412e+00  4.8745279e+00\n",
            "  -4.2515674e+00  3.3727255e+00 -1.3532119e+00]\n",
            " [-1.7432919e-01 -1.7122381e+00 -1.9791726e+00  3.9274247e+00\n",
            "   1.9716474e+00  1.5887187e+00  9.2802560e-01 -6.8637943e-01\n",
            "   1.7850884e+00  1.6352710e+00  1.2801430e+00  1.0334314e+00\n",
            "  -2.6498027e+00  3.6340708e-01  1.3076391e+00]\n",
            " [ 2.5086751e+00  1.3487464e+00  1.4984881e+00  5.5314841e+00\n",
            "   1.9824599e-01 -1.9394042e+00 -1.9735855e+00  1.7470669e+00\n",
            "   2.0773673e+00  6.3833427e-01 -4.4588723e+00  2.7985077e+00\n",
            "  -3.0717144e+00  8.2171535e-01  2.9586911e+00]\n",
            " [ 4.8616562e+00  3.5845511e+00  3.5750206e+00  2.7865317e+00\n",
            "  -3.0030954e-01 -6.4652383e-01 -5.5053506e+00  3.6965048e+00\n",
            "   2.9671733e+00 -1.0680034e-01  2.0595193e-01  2.4487860e+00\n",
            "   2.8677952e+00  2.5129747e+00 -2.2432690e+00]\n",
            " [ 3.9645231e+00  4.2334509e+00  4.6271901e+00  3.1230819e+00\n",
            "  -2.4836607e+00 -4.0450535e+00 -8.0176163e+00  2.7682157e+00\n",
            "   6.5767318e-01 -2.4576235e+00 -8.1818085e+00  1.2865710e-01\n",
            "   8.1807244e-01 -1.1674203e+00  3.5461462e+00]\n",
            " [ 3.6359167e+00  2.1230521e+00  1.6844466e+00  3.6577325e+00\n",
            "   9.4959080e-01 -7.7002496e-01 -1.6189179e+00  3.1154301e+00\n",
            "   2.8423169e+00  1.2749983e+00 -2.9295099e-01  4.0057192e+00\n",
            "  -3.6267543e-01  3.4360561e+00 -1.6664131e+00]]\n"
          ]
        }
      ]
    }
  ]
}